{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAkjzxwU-PEf",
        "outputId": "014bdcfa-3a81-4dcf-d61b-93a2faf18466"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "import os\n",
        "# drive.mount('/content/gdrive')\n",
        "import random\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtmRLT2N-XA4",
        "outputId": "f0c54136-7469-45c1-9252-032a99a735f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of train: 300001\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['605, Vạn Kiếp, Xã Lý Nhơn, Quận 10, Thành Phố Hồ Chí Minh',\n",
              " '479, Đường Số 15, Phường 04, Quận 4, Thành Phố Hồ Chí Minh',\n",
              " '125, Đường Số 3, Phường Tân Thuận Tây, Quận 5, Thành Phố Hồ Chí Minh',\n",
              " '988 Hoàng Diệu Phường 05 Quận 10 Thành Phố Hồ Chí Minh',\n",
              " '732, Đường Số 5, Xã Đa Phước, Quận Thủ Đức, Thành Phố Hồ Chí Minh',\n",
              " '755 Đường Số 38 Phường 03 Quận 2 Thành Phố Hồ Chí Minh',\n",
              " '299, Đường Số 702, Xã Tân Thạnh Đông, Quận 10, Thành Phố Hồ Chí Minh',\n",
              " '757 Đường Số 59 Xã Tân Thông Hội Quận Tân Bình Thành Phố Hồ Chí Minh',\n",
              " '955, Đường Vĩnh Nam, Phường 07, Quận 8, Thành Phố Hồ Chí Minh',\n",
              " '170 Đường 37A khu C Xã Vĩnh Lộc B Huyện Hóc Môn Thành Phố Hồ Chí Minh']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def _data_train(fn):\n",
        "  with open(fn, 'r') as fn:\n",
        "    train = fn.readlines()\n",
        "  train = [item[:-1] for item in train[:50000]]  \n",
        "  return train\n",
        "\n",
        "# with open(\"/content/gdrive/MyDrive/Test Internship TOT/Address Vietnam/data/true_address_VN.txt\", \"r\") as f_r:\n",
        "with open(\"true_address_VN.txt\", \"r\", encoding = 'utf-8') as f_r:\n",
        "    data = f_r.read().split(\"\\n\")\n",
        "# data = _data_train(fn = '/content/gdrive/MyDrive/Test Internship TOT/Address Vietnam/data/true_address_VN.txt')\n",
        "# _data = _data_train(fn = '/content/gdrive/MyDrive/Test Internship TOT/Address Vietnam/data/part_of_address.txt')\n",
        "# data = []\n",
        "# for x in _data:\n",
        "#   x = x[11:]\n",
        "#   data.append(x)\n",
        "# _data = []\n",
        "# data = data*100\n",
        "random.shuffle(data)\n",
        "print('length of train: {}'.format(len(data)))  \n",
        "data[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YLnBffcbkT22"
      },
      "outputs": [],
      "source": [
        "# # load the data\n",
        "# import pickle\n",
        "# data = pickle.load(open('./data/VNTC_data.pkl', 'rb'))\n",
        "# print(len(data),len(set(i for i in data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-0uHI6Z3kyt9",
        "outputId": "480dd98b-2d04-401b-c9a0-375b307372d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'94, Đường Nguyễn Xuân Khoát, Phường Thạnh Lộc, Quận 9, Thành Phố Hồ Chí Minh'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6nkAv_a6rOAJ",
        "outputId": "a2da4951-fcbe-49a1-be13-2b4f688ec80d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_11224\\393703432.py:6: FutureWarning: Possible nested set at position 2\n",
            "  s = re.sub(r, r\" \\1 \", s)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'112 , Đường Bờ Nhà Thờ , Phường Hiệp Phú , Quận 3 , Thành phố Hồ Chí Minh'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tách dấu ra khỏi từ\n",
        "def normalizeString(s):\n",
        "    # Tách dấu câu nếu kí tự liền nhau\n",
        "    marks = '[.!?,-${}()]'\n",
        "    r = \"([\"+\"\\\\\".join(marks)+\"])\"\n",
        "    s = re.sub(r, r\" \\1 \", s)\n",
        "    # Thay thế nhiều spaces bằng 1 space.\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "normalizeString('112, Đường Bờ Nhà Thờ, Phường Hiệp Phú, Quận 3, Thành phố Hồ Chí Minh')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lduTZ4kSrmA5"
      },
      "outputs": [],
      "source": [
        "data = [normalizeString(item) for item in data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZx3wCtlrr4u",
        "outputId": "2a0a2ba5-6b4f-4f89-c968-a9bab68e1883"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['605 , Vạn Kiếp , Xã Lý Nhơn , Quận 10 , Thành Phố Hồ Chí Minh',\n",
              " '479 , Đường Số 15 , Phường 04 , Quận 4 , Thành Phố Hồ Chí Minh',\n",
              " '125 , Đường Số 3 , Phường Tân Thuận Tây , Quận 5 , Thành Phố Hồ Chí Minh']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJtTEdksaIGR",
        "outputId": "7f978ccf-5d7c-472c-fb0f-c674e25f28ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "299387\n"
          ]
        }
      ],
      "source": [
        "# extract Latin- characters only\n",
        "import re\n",
        "alphabet = '^[ _abcdefghijklmnopqrstuvwxyz0123456789áàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđ!\\\"\\',\\-\\.:;?_\\(\\)]+$'\n",
        "training_data=[]\n",
        "for i in data:\n",
        "  i=i.replace(\"\\n\",\".\")\n",
        "  sentences=i.split(\".\")\n",
        "  for j in sentences:\n",
        "      if len(j.split()) > 2 and re.match(alphabet, j.lower()):\n",
        "          training_data.append(j)\n",
        "print(len(training_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HrPysUyuX2x-"
      },
      "outputs": [],
      "source": [
        "del data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gI8pYWxpKLwg",
        "outputId": "3f0bd558-f707-48fb-bd13-292d926d22a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['382 , Thạnh Lộc 16 , Phường Tân Sơn Nhì , Quận Bình Tân , Thành Phố Hồ Chí Minh', '328 , Trần Văn Giáp , Xã Đông Thạnh , Huyện Cần Giờ , Thành Phố Hồ Chí Minh', '56 , Đường Võ Trứ , Phường 25 , Quận 8 , Thành Phố Hồ Chí Minh', '394 Bùi Quốc Khái Phường 07 Quận Thủ Đức Thành Phố Hồ Chí Minh', '220 , Đường Trần Quốc Hoàn , Phường Thảo Điền , Quận 9 , Thành Phố Hồ Chí Minh', '152 , Đường Số 46 , Phường Tân Thành , Huyện Củ Chi , Thành Phố Hồ Chí Minh', '462 , Đường Đường C7 , Phường Tân Quy , Quận Tân Bình , Thành Phố Hồ Chí Minh', '998 Đại Nghĩa Phường 06 Quận 6 Thành Phố Hồ Chí Minh', '546 , Thông Tây Hội , Phường 11 , Huyện Hóc Môn , Thành Phố Hồ Chí Minh', '553 , Đường Đường Số 623 , Phường 05 , Quận Tân Phú , Thành Phố Hồ Chí Minh']\n"
          ]
        }
      ],
      "source": [
        "print(training_data[-10:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj4mI7jXOdel",
        "outputId": "2bf27176-ed96-4477-ab89-541de2e6eeb8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#'\n",
            "WARNING: There was an error checking the latest version of pip.\n"
          ]
        }
      ],
      "source": [
        "!pip install unidecode # this module removes tones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "e-R0-m3KeBWk"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from unidecode import unidecode\n",
        "# some common Vietnamese spell mistake\n",
        "letters=list(\"abcdefghijklmnopqrstuvwxyzáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđABCDEFGHIJKLMNOPQRSTUVWXYZÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÉÈẺẼẸÊẾỀỂỄỆÚÙỦŨỤƯỨỪỬỮỰÍÌỈĨỊÝỲỶỸỴĐ\")\n",
        "letters2=list(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "\n",
        "typo={\"ă\":\"aw\",\"â\":\"aa\",\"á\":\"as\",\"à\":\"af\",\"ả\":\"ar\",\"ã\":\"ax\",\"ạ\":\"aj\",\"ắ\":\"aws\",\"ổ\":\"oor\",\"ỗ\":\"oox\",\"ộ\":\"ooj\",\"ơ\":\"ow\",\n",
        "\"ằ\":\"awf\",\"ẳ\":\"awr\",\"ẵ\":\"awx\",\"ặ\":\"awj\",\"ó\":\"os\",\"ò\":\"of\",\"ỏ\":\"or\",\"õ\":\"ox\",\"ọ\":\"oj\",\"ô\":\"oo\",\"ố\":\"oos\",\"ồ\":\"oof\",\n",
        "\"ớ\":\"ows\",\"ờ\":\"owf\",\"ở\":\"owr\",\"ỡ\":\"owx\",\"ợ\":\"owj\",\"é\":\"es\",\"è\":\"ef\",\"ẻ\":\"er\",\"ẽ\":\"ex\",\"ẹ\":\"ej\",\"ê\":\"ee\",\"ế\":\"ees\",\"ề\":\"eef\",\n",
        "\"ể\":\"eer\",\"ễ\":\"eex\",\"ệ\":\"eej\",\"ú\":\"us\",\"ù\":\"uf\",\"ủ\":\"ur\",\"ũ\":\"ux\",\"ụ\":\"uj\",\"ư\":\"uw\",\"ứ\":\"uws\",\"ừ\":\"uwf\",\"ử\":\"uwr\",\"ữ\":\"uwx\",\n",
        "\"ự\":\"uwj\",\"í\":\"is\",\"ì\":\"if\",\"ỉ\":\"ir\",\"ị\":\"ij\",\"ĩ\":\"ix\",\"ý\":\"ys\",\"ỳ\":\"yf\",\"ỷ\":\"yr\",\"ỵ\":\"yj\",\"đ\":\"dd\",\n",
        "\"Ă\":\"Aw\",\"Â\":\"Aa\",\"Á\":\"As\",\"À\":\"Af\",\"Ả\":\"Ar\",\"Ã\":\"Ax\",\"Ạ\":\"Aj\",\"Ắ\":\"Aws\",\"Ổ\":\"Oor\",\"Ỗ\":\"Oox\",\"Ộ\":\"Ooj\",\"Ơ\":\"Ow\",\n",
        "\"Ằ\":\"AWF\",\"Ẳ\":\"Awr\",\"Ẵ\":\"Awx\",\"Ặ\":\"Awj\",\"Ó\":\"Os\",\"Ò\":\"Of\",\"Ỏ\":\"Or\",\"Õ\":\"Ox\",\"Ọ\":\"Oj\",\"Ô\":\"Oo\",\"Ố\":\"Oos\",\"Ồ\":\"Oof\",\n",
        "\"Ớ\":\"Ows\",\"Ờ\":\"Owf\",\"Ở\":\"Owr\",\"Ỡ\":\"Owx\",\"Ợ\":\"Owj\",\"É\":\"Es\",\"È\":\"Ef\",\"Ẻ\":\"Er\",\"Ẽ\":\"Ex\",\"Ẹ\":\"Ej\",\"Ê\":\"Ee\",\"Ế\":\"Ees\",\"Ề\":\"Eef\",\n",
        "\"Ể\":\"Eer\",\"Ễ\":\"Eex\",\"Ệ\":\"Eej\",\"Ú\":\"Us\",\"Ù\":\"Uf\",\"Ủ\":\"Ur\",\"Ũ\":\"Ux\",\"Ụ\":\"Uj\",\"Ư\":\"Uw\",\"Ứ\":\"Uws\",\"Ừ\":\"Uwf\",\"Ử\":\"Uwr\",\"Ữ\":\"Uwx\",\n",
        "\"Ự\":\"Uwj\",\"Í\":\"Is\",\"Ì\":\"If\",\"Ỉ\":\"Ir\",\"Ị\":\"Ij\",\"Ĩ\":\"Ix\",\"Ý\":\"Ys\",\"Ỳ\":\"Yf\",\"Ỷ\":\"Yr\",\"Ỵ\":\"Yj\",\"Đ\":\"Dd\"}\n",
        "\n",
        "region={\"ẻ\":\"ẽ\",\"ẽ\":\"ẻ\",\"ũ\":\"ủ\",\"ủ\":\"ũ\",\"ã\":\"ả\",\"ả\":\"ã\",\"ỏ\":\"õ\",\"õ\":\"ỏ\",\"i\":\"j\"}\n",
        "region2={\"s\":\"x\",\"l\":\"n\",\"n\":\"l\",\"x\":\"s\",\"d\":\"gi\",\"S\":\"X\",\"L\":\"N\",\"N\":\"L\",\"X\":\"S\",\"Gi\":\"D\",\"D\":\"Gi\"}\n",
        "\n",
        "vowel=list(\"aeiouyáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵ\")\n",
        "acronym={\"không\":\"ko\",\" anh\":\" a\",\"em\":\"e\",\"biết\":\"bít\",\"giờ\":\"h\",\"gì\":\"j\",\"muốn\":\"mún\",\"học\":\"hok\",\"yêu\":\"iu\",\n",
        "         \"chồng\":\"ck\",\"vợ\":\"vk\",\" ông\":\" ô\",\"được\":\"đc\",\"tôi\":\"t\",\n",
        "         \"Không\":\"Ko\",\" Anh\":\" A\",\"Em\":\"E\",\"Biết\":\"Bít\",\"Giờ\":\"H\",\"Gì\":\"J\",\"Muốn\":\"Mún\",\"Học\":\"Hok\",\"Yêu\":\"Iu\",\n",
        "         \"Chồng\":\"Ck\",\"Vợ\":\"Vk\",\" Ông\":\" Ô\",\"Được\":\"Đc\",\"Tôi\":\"T\",}\n",
        "\n",
        "teen={\"ch\":\"ck\",\"ph\":\"f\",\"th\":\"tk\",\"nh\":\"nk\",\n",
        "      \"Ch\":\"Ck\",\"Ph\":\"F\",\"Th\":\"Tk\",\"Nh\":\"Nk\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xRyDFpS0_cBv"
      },
      "outputs": [],
      "source": [
        "# function for adding mistake( noise)\n",
        "def teen_code(sentence,pivot):\n",
        "    random = np.random.uniform(0,1,1)[0]\n",
        "    new_sentence=str(sentence)\n",
        "    if random>pivot:\n",
        "        for word in acronym.keys():\n",
        "            if re.search(word, new_sentence):\n",
        "                random2 = np.random.uniform(0,1,1)[0]\n",
        "                if random2 <0.5:\n",
        "                    new_sentence=new_sentence.replace(word,acronym[word])\n",
        "        for word in teen.keys(): \n",
        "            if re.search(word, new_sentence):\n",
        "                random3 = np.random.uniform(0,1,1)[0]\n",
        "                if random3 <0.05:\n",
        "                    new_sentence=new_sentence.replace(word,teen[word])        \n",
        "        return new_sentence\n",
        "    else:\n",
        "        return sentence\n",
        "    \n",
        "\n",
        "def add_noise(sentence, pivot1,pivot2):\n",
        "    sentence=teen_code(sentence,0.5)\n",
        "    noisy_sentence = \"\"\n",
        "    i = 0\n",
        "    while i < len(sentence):\n",
        "        if sentence[i] not in letters:\n",
        "            noisy_sentence+=sentence[i]\n",
        "        else: \n",
        "            random = np.random.uniform(0,1,1)[0]   \n",
        "            if random < pivot1:\n",
        "                noisy_sentence+=(sentence[i])\n",
        "            elif random<pivot2:\n",
        "                if sentence[i] in typo.keys() and sentence[i] in region.keys():\n",
        "                    random2=np.random.uniform(0,1,1)[0]\n",
        "                    if random2<=0.4:\n",
        "                        noisy_sentence+=typo[sentence[i]]\n",
        "                    elif random2<0.8:\n",
        "                        noisy_sentence+=region[sentence[i]]\n",
        "                    elif random2<0.95 :\n",
        "                        noisy_sentence+=unidecode(sentence[i])\n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "                elif sentence[i] in typo.keys():\n",
        "                    random3=np.random.uniform(0,1,1)[0]\n",
        "                    if random3<=0.6:\n",
        "                        noisy_sentence+=typo[sentence[i]]\n",
        "                    elif random3<0.9 :\n",
        "                        noisy_sentence+=unidecode(sentence[i])                        \n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "                elif sentence[i] in region.keys():\n",
        "                    random4=np.random.uniform(0,1,1)[0]\n",
        "                    if random4<=0.6:\n",
        "                        noisy_sentence+=region[sentence[i]]\n",
        "                    elif random4<0.85 :\n",
        "                        noisy_sentence+=unidecode(sentence[i])                        \n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "                elif i<len(sentence)-1 :\n",
        "                    if sentence[i] in region2.keys() and (i==0 or sentence[i-1] not in letters) and sentence[i+1] in vowel:\n",
        "                        random5=np.random.uniform(0,1,1)[0]\n",
        "                        if random5<=0.9:\n",
        "                            noisy_sentence+=region2[sentence[i]]\n",
        "                        else:\n",
        "                            noisy_sentence+=sentence[i]\n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "\n",
        "            else:\n",
        "                new_random = np.random.uniform(0,1,1)[0]\n",
        "                if new_random <=0.33:\n",
        "                    if i == (len(sentence) - 1):\n",
        "                        continue\n",
        "                    else:\n",
        "                        noisy_sentence+=(sentence[i+1])\n",
        "                        noisy_sentence+=(sentence[i])\n",
        "                        i += 1\n",
        "                elif new_random <= 0.66:\n",
        "                    random_letter = np.random.choice(letters2, 1)[0]\n",
        "                    noisy_sentence+=random_letter\n",
        "                else:\n",
        "                    pass\n",
        "      \n",
        "        i += 1\n",
        "    return noisy_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wfWD5D-DeEXW"
      },
      "outputs": [],
      "source": [
        "def extract_phrases(text):\n",
        "    return re.findall(r'\\w[\\w ]+', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9Oas0TPeLXb",
        "outputId": "6b85e9b5-eab2-4422-c37d-7750c6a6d885"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "900904\n",
            "['Thành Phố Hồ Chí Minh', '998 Đại Nghĩa Phường 06 Quận 6 Thành Phố Hồ Chí Minh', 'Thông Tây Hội', 'Phường 11', 'Huyện Hóc Môn', 'Thành Phố Hồ Chí Minh', 'Đường Đường Số 623', 'Phường 05', 'Quận Tân Phú', 'Thành Phố Hồ Chí Minh']\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "phrases = itertools.chain.from_iterable(extract_phrases(text) for text in training_data)\n",
        "phrases = [p.strip() for p in phrases if len(p.split()) > 1]\n",
        "\n",
        "print(len(phrases))\n",
        "print(phrases[-10:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdyNMyEZecv2",
        "outputId": "71aca7c7-7080-4b8d-cbdd-08467d0f4747"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 900904/900904 [00:02<00:00, 326181.28it/s]\n"
          ]
        }
      ],
      "source": [
        "from nltk import ngrams\n",
        "import string\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# divide document into 5-grams \n",
        "# a single Vietnamese word cant contain more than 7 characters ( nghiêng )\n",
        "NGRAM = 4 \n",
        "MAXLEN = 31\n",
        "def gen_ngrams(words, n=4):\n",
        "    return ngrams(words.split(), n)\n",
        "   \n",
        "list_ngrams = []\n",
        "for p in tqdm(phrases):\n",
        "  list_p = p.split()\n",
        "  if len(list_p) >= NGRAM:\n",
        "    for ngr in ngrams(p.split(), NGRAM):\n",
        "      if len(\" \".join(ngr)) < MAXLEN:\n",
        "        list_ngrams.append(\" \".join(ngr))\n",
        "  elif len(' '.join(list_p)) <= MAXLEN:\n",
        "    list_ngrams.append(\" \".join(list_p))\n",
        "# for p in tqdm(phrases):\n",
        "#   if not re.match(alphabet, p.lower()):\n",
        "#     continue\n",
        "#   for ngr in gen_ngrams(p, NGRAM):\n",
        "#     if len(\" \".join(ngr)) < MAXLEN:\n",
        "#       list_ngrams.append(\" \".join(ngr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLdGXkwunTWL",
        "outputId": "690cc55b-fc4f-45dd-aaed-91a5f5888b6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2106347\n"
          ]
        }
      ],
      "source": [
        "del phrases\n",
        "list_ngrams = list((list_ngrams))\n",
        "print(len(list_ngrams))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfY56WCmh9q4",
        "outputId": "64f66110-eade-431f-fd4a-a2455887e9e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n",
            "199\n"
          ]
        }
      ],
      "source": [
        "alphabet = ['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n",
        "print(alphabet)\n",
        "print(len(alphabet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "hdYUSow2h_dE"
      },
      "outputs": [],
      "source": [
        "# So a 5-grams contain at most 7*5 = 35 character (except one that has spell mistake)\n",
        "# add \"\\x00\" padding at the end of 5-grams in order to equal their length\n",
        "def encoder_data(text, maxlen=MAXLEN):\n",
        "        # text = \"\\x00\" + text\n",
        "        x = np.zeros((maxlen, len(alphabet)))\n",
        "        for i, c in enumerate(text[:maxlen]):\n",
        "            x[i, alphabet.index(c)] = 1\n",
        "        if i < maxlen - 1:\n",
        "          for j in range(i+1, maxlen):\n",
        "            x[j, 0] = 1\n",
        "        return x\n",
        "      \n",
        "def decoder_data(x):\n",
        "    x = x.argmax(axis=-1)\n",
        "    return ''.join(alphabet[i] for i in x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW-UumbH1O_H",
        "outputId": "caa879ff-8a04-474c-f9c5-9e2d8a8b0b3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(31, 199)\n",
            "Tôi tên là Thái Linh\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n"
          ]
        }
      ],
      "source": [
        "print(encoder_data(\"Tôi tên là thái linh\").shape)\n",
        "print(decoder_data(encoder_data(\"Tôi tên là Thái Linh\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "tVYTzGwVr9DY"
      },
      "outputs": [],
      "source": [
        "# Build the neural network\n",
        "# this is adapted from the seq2seq architecture, which can be used for Machine Translation, Text Summarization Image Captioning ...\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, TimeDistributed, Dense,LSTM, Bidirectional\n",
        "from keras.callbacks import Callback, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "m3bwCnPSsEBL"
      },
      "outputs": [],
      "source": [
        "encoder=LSTM(256,input_shape=(MAXLEN, len(alphabet)), return_sequences=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Z_hJ1B0rshuF"
      },
      "outputs": [],
      "source": [
        "decoder=Bidirectional(LSTM(256, return_sequences=True, dropout=0.2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "QEGAP0kvx_dD"
      },
      "outputs": [],
      "source": [
        "model=Sequential()\n",
        "model.add(encoder)\n",
        "model.add(decoder)\n",
        "model.add(TimeDistributed(Dense(256)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(TimeDistributed(Dense(len(alphabet))))\n",
        "model.add(Activation('softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcYYVoNdfzjs",
        "outputId": "6a92174c-d4d1-4d7b-c46e-209062e84d9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 31, 256)           466944    \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 31, 512)          1050624   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 31, 256)          131328    \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 31, 256)           0         \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, 31, 199)          51143     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 31, 199)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,700,039\n",
            "Trainable params: 1,700,039\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "hxS4ht05G5Zk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "c6wpr3lxiHZK"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data, valid_data = train_test_split(list_ngrams, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "sMYZdVyuUPoG"
      },
      "outputs": [],
      "source": [
        "del list_ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_8WAhdlXiPZV"
      },
      "outputs": [],
      "source": [
        "# we have to use data- generation medthod cause this dataset is too large to fit into memory\n",
        "BATCH_SIZE = 2048\n",
        "def generate_data(data, batch_size):\n",
        "    cur_index = 0\n",
        "    while True:\n",
        "        x, y = [], []\n",
        "        for i in range(batch_size):  \n",
        "            y.append(encoder_data(data[cur_index]))\n",
        "            x.append(encoder_data(add_noise(data[cur_index],0.94,0.985)))\n",
        "            cur_index += 1\n",
        "            if cur_index > len(data)-1:\n",
        "                cur_index = 0\n",
        "        yield np.array(x), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "wyqr-laHiSm1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "train_generator = generate_data(train_data, batch_size=BATCH_SIZE)\n",
        "validation_generator = generate_data(valid_data, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn50tT-jjD4-",
        "outputId": "b7fc32b0-64bd-4d61-fb3f-2b0af211a5a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_11224\\2798714209.py:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  X = model.fit_generator(train_generator, steps_per_epoch=len(train_data)//BATCH_SIZE, epochs=1,\n"
          ]
        }
      ],
      "source": [
        "# train the model and save to the Model folder\n",
        "# checkpointer = ModelCheckpoint(filepath=os.path.join('/model/spell_{val_acc:.2f}.h5'), save_best_only=True, verbose=1)\n",
        "\n",
        "X = model.fit_generator(train_generator, steps_per_epoch=len(train_data)//BATCH_SIZE, epochs=1,\n",
        "                    validation_data=validation_generator, validation_steps=len(valid_data)//BATCH_SIZE) #callbacks=[checkpointer])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92_EINxdHbWZ"
      },
      "outputs": [],
      "source": [
        "model.save('model_spell{0:.4f}.h5'.format(X.history['val_accuracy'][-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIxaZ2hdB1mq"
      },
      "outputs": [],
      "source": [
        "# X = model.fit_generator()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train_spell.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "31671a60cee805c34c73116577b485118ff3a75c458d3004d49632c19702ac60"
    },
    "kernelspec": {
      "display_name": "Python 3.10.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
